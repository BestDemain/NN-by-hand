# 手工实现三层神经网络分类器

本项目从零开始实现一个三层神经网络分类器，用于CIFAR-10数据集的图像分类任务。项目不使用任何自动微分框架（如PyTorch或TensorFlow），而是手动实现反向传播算法，通过NumPy手动实现了神经网络的前向传播、反向传播、各种激活函数、损失函数和优化器等核心组件。

具体原理说明见[项目说明](项目说明.ipynb)。

完整实验见[main_notebook](main_notebook.ipynb)或[main](main.py)。

简单版实验见[实验](实验报告.ipynb)。

实验报告见[model_report](model_report.md)。

## 1. 项目概述

本项目实现了一个完整的三层神经网络分类器，包括：

- **模型架构**：输入层 → 隐藏层1 → 隐藏层2 → 输出层
- **激活函数**：支持多种激活函数（Sigmoid、ReLU、Tanh、Softmax）
- **损失函数**：交叉熵损失函数
- **优化器**：SGD优化器（支持动量和L2正则化）
- **学习率调度**：支持学习率衰减策略
- **超参数搜索**：支持网格搜索和随机搜索两种方法

项目的主要目标是通过手动实现神经网络的各个组件，深入理解神经网络的工作原理和训练过程。

## 2. 项目结构

项目文件组织如下：

### 2.1 核心模型组件

```
├── model.py          # 神经网络模型定义，包含前向传播和反向传播
├── activations.py    # 激活函数及其导数的实现
├── losses.py         # 损失函数（交叉熵）及其导数的实现
├── optimizer.py      # SGD优化器实现，包含学习率调度和L2正则化
```

### 2.2 训练与评估组件

```
├── train.py          # 训练逻辑，包含模型训练和验证
├── test.py           # 测试逻辑，用于评估模型性能
├── evaluation.py     # 模型评估指标和方法
├── hyperparameter_search.py  # 超参数搜索实现
```

### 2.3 工具与配置

```
├── utils.py          # 工具函数，包括数据加载和预处理
├── config.py         # 配置文件，包含默认超参数
├── visualize_weights.py  # 权重可视化工具
├── visualize_training.py # 训练过程可视化工具
```

### 2.4 主程序与结果

```
├── main.py           # 主程序入口
├── main_notebook.ipynb  # Jupyter notebook版本的主程序
├── results/          # 结果文件夹，包含训练历史、权重可视化等
└── models/           # 模型保存文件夹
```

## 3. 模型介绍

### 3.1 模型架构

模型采用了经典的三层神经网络结构：

- **输入层**：接收CIFAR-10图像数据，维度为3072（32×32×3）
- **第一隐藏层**：可配置的神经元数量，默认为512个神经元
- **第二隐藏层**：可配置的神经元数量，默认为256个神经元
- **输出层**：10个神经元，对应CIFAR-10的10个类别

### 3.2 默认超参数配置

模型的默认超参数配置如下：

#### 3.2.1 模型参数

- **输入大小(input_size)**: 3072 (32×32×3，CIFAR-10图像展平后的维度)
- **第一隐藏层大小(hidden_size1)**: 512个神经元
- **第二隐藏层大小(hidden_size2)**: 256个神经元
- **输出大小(output_size)**: 10 (CIFAR-10的10个类别)
- **隐藏层激活函数(hidden_activation)**: ReLU
- **输出层激活函数(output_activation)**: Softmax

#### 3.2.2 训练参数

- **损失函数(loss)**: 交叉熵(cross_entropy)
- **优化器(optimizer)**: SGD
- **学习率(learning_rate)**: 0.2
- **动量系数(momentum)**: 0.9
- **权重衰减(weight_decay)**: 0.0001 (L2正则化系数)
- **批量大小(batch_size)**: 256
- **训练轮数(epochs)**: 25

#### 3.2.3 学习率调度

- **调度器类型(scheduler)**: step (步长衰减)
- **调度器参数(scheduler_params)**:
  - **step_size**: 1 (每1个epoch调整一次学习率)
  - **gamma**: 0.98 (每次调整将学习率乘以0.98)

### 3.3 激活函数

- **隐藏层**：支持多种激活函数，包括ReLU、Sigmoid和Tanh
  - ReLU: $f(x) = \max(0, x)$
  - Sigmoid: $f(x) = \frac{1}{1 + e^{-x}}$
  - Tanh: $f(x) = \tanh(x)$
- **输出层**：使用Softmax激活函数，将输出转换为概率分布

### 3.4 损失函数

使用交叉熵损失函数，适用于多分类问题：
$L = -\sum_{i} y_{true,i} \log(y_{pred,i})$

### 3.5 优化器

使用带动量的随机梯度下降（SGD）优化器，并支持L2正则化以防止过拟合：

- 速度更新：$v = \beta \cdot v - \alpha \cdot \nabla J(\theta)$
- 参数更新：$\theta = \theta + v$

其中，$\beta$是动量系数（默认为0.9），$\alpha$是学习率（默认为0.2），$\nabla J(\theta)$是损失函数关于参数$\theta$的梯度。

### 3.6 参数初始化

使用Xavier/Glorot初始化方法初始化权重，有助于解决训练初期的梯度消失/爆炸问题。

## 4. 数据集介绍

### 4.1 CIFAR-10数据集

CIFAR-10是一个广泛使用的计算机视觉数据集，包含10个类别的60,000张32×32彩色图像：

- 飞机（airplane）
- 汽车（automobile）
- 鸟（bird）
- 猫（cat）
- 鹿（deer）
- 狗（dog）
- 青蛙（frog）
- 马（horse）
- 船（ship）
- 卡车（truck）

数据集被分为50,000张训练图像和10,000张测试图像。

### 4.2 数据预处理

数据预处理步骤包括：

1. **归一化**：将像素值从[0, 255]归一化到[0, 1]范围
2. **标准化**：对数据进行零均值、单位方差的标准化处理
3. **数据分割**：将训练集进一步分割为训练集和验证集（90%训练，10%验证）

## 5. 功能特点

### 5.1 模型部分

- 可自定义隐藏层大小
- 支持多种激活函数（Sigmoid, ReLU, Tanh等）
- 手动实现反向传播算法

### 5.2 训练部分

- 实现SGD优化器
- 支持学习率下降策略
- 实现交叉熵损失函数
- 支持L2正则化
- 根据验证集性能自动保存最优模型

### 5.3 超参数搜索

项目实现了两种超参数搜索方法，用于寻找最佳的模型配置：

#### 5.3.1 网格搜索

网格搜索通过遍历所有可能的超参数组合来寻找最佳配置：

```python
# 网格搜索示例
if search_type == 'grid':
    print("执行网格搜索...")
    best_config, results = example_grid_search(
        X_train, y_train, X_val, y_val, X_test, y_test
    )
```

网格搜索参数空间：

- **第一隐藏层大小(hidden_size1)**: [128, 256, 512]
- **第二隐藏层大小(hidden_size2)**: [64, 128, 256]
- **隐藏层激活函数(hidden_activation)**: ['relu', 'tanh']
- **学习率(learning_rate)**: [0.1, 0.01, 0.001]
- **动量系数(momentum)**: [0.0, 0.9]
- **权重衰减(weight_decay)**: [0.0, 0.0001, 0.001]

#### 5.3.2 随机搜索

随机搜索通过随机采样超参数空间来寻找最佳配置，计算效率更高：

```python
# 随机搜索示例
else:
    print("执行随机搜索...")
    best_config, results = example_random_search(
        X_train, y_train, X_val, y_val, X_test, y_test
    )
```

随机搜索参数空间：

- **第一隐藏层大小(hidden_size1)**: [128, 256, 512, 1024]
- **第二隐藏层大小(hidden_size2)**: [64, 128, 256, 512]
- **隐藏层激活函数(hidden_activation)**: ['relu', 'tanh']
- **学习率(learning_rate)**: 对数均匀分布在10^(-4)到10^(-1)之间
- **动量系数(momentum)**: 均匀分布在0到0.99之间
- **权重衰减(weight_decay)**: 对数均匀分布在10^(-5)到10^(-2)之间

随机搜索配置：

- **迭代次数(n_iter)**: 10 (尝试10种不同的超参数组合)
- **每次搜索的训练轮数(epochs)**: 10

### 5.4 测试部分

- 支持导入训练好的模型
- 计算并输出测试集上的分类准确率

## 6. 实验结果

### 6.1 最佳配置

通过超参数搜索，找到了以下最佳配置：

- **第一隐藏层**：512个神经元
- **第二隐藏层**：256个神经元
- **隐藏层激活函数**：ReLU
- **学习率**：0.01
- **动量系数**：0.9
- **权重衰减**：0.0001

### 6.2 模型性能

最佳模型在验证集和测试集上的性能：

- **验证集准确率**：约49%
- **测试集准确率**：约49%

这个性能虽然不及现代深度卷积神经网络（可达90%以上），但对于一个从零实现的简单三层全连接网络来说是合理的，因为它没有利用图像的空间结构信息。

### 6.3 可视化结果

项目包含多种可视化结果，存储在 `results/`文件夹中：

- **训练历史**：损失曲线和准确率曲线
- **权重可视化**：第一层权重矩阵可视化
- **权重分布**：各层权重的分布直方图
- **类别准确率**：不同类别的分类准确率

## 7. 使用方法

### 7.1 训练模型

```bash
python main.py --mode train
```

可选参数：

- `--learning_rate`：学习率
- `--hidden_size1`：第一隐藏层大小
- `--hidden_size2`：第二隐藏层大小
- `--batch_size`：批量大小
- `--epochs`：训练轮数
- `--momentum`：动量系数
- `--weight_decay`：L2正则化强度
- `--activation`：激活函数类型

### 7.2 测试模型

```bash
python main.py --mode test --model_path path/to/model
```

### 7.3 超参数搜索

```bash
python main.py --mode search
```

## 8. 结论与改进方向

### 8.1 结论

本项目成功实现了一个从零开始的三层神经网络分类器，并应用于CIFAR-10图像分类任务。通过超参数搜索找到了较优的模型配置，并通过可视化分析了训练过程和模型参数。

主要成果：

- 实现了完整的神经网络训练流程，包括前向传播、反向传播、参数更新等
- 实现了多种激活函数、损失函数和优化器
- 通过超参数搜索提高了模型性能
- 通过可视化深入分析了模型的训练过程和学习到的特征

### 8.2 改进方向

尽管本项目成功实现了基本的神经网络，但仍有多个可改进的方向：

1. **模型架构**：

   - 使用卷积神经网络（CNN）替代全连接网络，更好地利用图像的空间结构
   - 增加网络深度，添加更多层以提高模型表达能力
   - 尝试残差连接（ResNet）等现代架构技术
2. **优化方法**：

   - 实现更先进的优化器，如Adam、RMSprop等
   - 添加Batch Normalization等正则化技术
   - 实现Dropout防止过拟合
3. **数据增强**：

   - 实现图像旋转、翻转、裁剪等数据增强技术
   - 使用更复杂的数据预处理方法

## 9. 依赖库

- NumPy：用于数学计算
- Matplotlib：用于可视化训练过程
- scikit-learn：用于数据处理和评估指标
